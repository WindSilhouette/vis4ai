// src/components/TutorialOverlay.jsx
import { useEffect, useState } from "react";

const steps = [
  {
    id: "welcome",
    title: "Welcome to the Reinforcement Learning 101 üëã",
    body: `
This page is a mini-course on reinforcement learning.

You‚Äôll watch TWO agents learn in the SAME environment:

‚Ä¢ Left panel: Tabular Q-Learning
‚Ä¢ Right panel: Deep Q-Learning (DQN)

Use the tabs at the very top to switch between:
‚Ä¢ CliffWalk ‚Äî a gridworld navigation task
‚Ä¢ CartPole ‚Äî a classic balancing control task

We‚Äôll walk through what RL is, how to read this interface, and
why choosing the wrong method for a problem can hurt performance.`,
    focusSide: "both",
    sectionId: "top-controls",
  },
  {
    id: "layout",
    title: "Layout of the Page",
    body: `
Look at the two big panels:

Each side (left and right) has the same structure:

1) Top of each panel:
   ‚Ä¢ Episode slider and Step slider (scrub through training)
2) Middle:
   ‚Ä¢ Environment visualization + current behavior/policy
3) Info cards:
   ‚Ä¢ Current state, chosen action, reward, and next state
4) Bottom:
   ‚Ä¢ The update rule and a reward-per-episode plot

Because both sides share the same layout, you can directly compare
how Q-Learning and DQN behave on the SAME task.`,
    focusSide: "both",
    sectionId: "panels-wrapper",
  },
  {
    id: "rl-core",
    title: "Reinforcement Learning in One Picture",
    body: `
Both sides visualize the core RL loop:

‚Ä¢ STATE s ‚Äî what the agent currently observes
‚Ä¢ ACTION a ‚Äî what it decides to do
‚Ä¢ REWARD r ‚Äî immediate feedback from the environment
‚Ä¢ NEXT STATE s' ‚Äî where it ends up after the action

Over many episodes, the agent tries to learn a POLICY:
a rule that maps states ‚Üí actions to maximize long-term reward.

Both Q-Learning and DQN learn an action-value function Q(s,a)
that estimates ‚Äúhow good‚Äù each action is in each state.`,
    focusSide: "both",
    sectionId: "panels-wrapper",
  },
  {
    id: "cliffwalk-env",
    title: "CliffWalk Environment (Discrete Grid)",
    body: `
Now focus on the LEFT panel and make sure the environment at the top
is set to "CliffWalk":

‚Ä¢ Each cell in the grid is a discrete STATE
‚Ä¢ Actions are ‚Üë ‚Üí ‚Üì ‚Üê
‚Ä¢ Blue = start, Yellow = goal, Red = cliff, Green = agent

Stepping into the cliff gives a big negative reward.
Every move costs a small negative reward, so the agent wants a
short, SAFE path along the edge.

This is a small, nicely discrete world ‚Äî perfect for a table-based
method like tabular Q-Learning.`,
    focusSide: "left",
    sectionId: "left-panel",
  },
  {
    id: "qlearning-good-match",
    title: "Why Q-Learning Fits CliffWalk",
    body: `
On the LEFT panel, Q-Learning stores Q(s,a) in a Q-TABLE.

Try this:

1) Move the Episode slider from early to late episodes.
2) Scrub the Step slider within an episode.

Watch how:

‚Ä¢ The arrows or colors in the grid stabilize into a shortest safe path
‚Ä¢ The Q-table under the grid (if shown) becomes more confident
‚Ä¢ The reward-per-episode plot trends upward and then stabilizes

Here, tabular Q-Learning is efficient and interpretable:
you can literally look up ‚Äúwhat the agent thinks‚Äù for each (s,a).`,
    focusSide: "left",
    sectionId: "left-panel",
  },
  {
    id: "dqn-on-cliffwalk",
    title: "DQN on CliffWalk: Overkill",
    body: `
Now look at the RIGHT panel while still on CliffWalk.

The environment is the SAME, but here Q(s,a) is approximated by
a neural network (DQN) instead of a table.

This has pros and cons:

‚Ä¢ Pros:
  ‚Äì Can scale to huge or continuous state spaces
  ‚Äì Can generalize between similar states

‚Ä¢ Cons on a tiny grid like CliffWalk:
  ‚Äì More parameters to tune
  ‚Äì Harder to interpret
  ‚Äì Risk of overfitting or instability for no real benefit

CliffWalk is a case where a simple table is usually the better choice:
using DQN ‚Äúworks‚Äù, but it‚Äôs more complexity than you need.`,
    focusSide: "right",
    sectionId: "right-panel",
  },
  {
    id: "switch-cartpole",
    title: "Switch to CartPole üïπÔ∏è",
    body: `
Now let‚Äôs see the opposite scenario.

üëâ At the very top of the page, CLICK the "CartPole" tab.

Both panels will switch to the CartPole environment.

CartPole state is continuous, typically something like:
[cart position, cart velocity, pole angle, pole angular velocity]

Actions are:
‚Ä¢ push cart left
‚Ä¢ push cart right

The goal is to keep the pole upright for as many time steps as possible.

Because the state is continuous, a naive Q-table either:
‚Ä¢ explodes in size, or
‚Ä¢ relies on very coarse discretization (losing information).`,
    focusSide: "both",
    sectionId: "top-controls",
  },
  {
    id: "qlearning-bad-match-cartpole",
    title: "Q-Learning Struggles on CartPole",
    body: `
Focus on the LEFT panel while CartPole is selected.

Try:

1) Move to early episodes ‚Äî behavior is usually very unstable.
2) Move to much later episodes ‚Äî performance may improve, but
   it often stays noisy or fragile.

Why? Because tabular Q-Learning has to discretize a continuous state:

‚Ä¢ Many states that are actually different get merged into the same bin
‚Ä¢ The agent cannot smoothly generalize to ‚Äúnearby‚Äù states
‚Ä¢ Small changes in angle/velocity can require very different actions

This is a case where the METHOD is a bad match for the PROBLEM.`,
    focusSide: "left",
    sectionId: "left-panel",
  },
  {
    id: "dqn-good-match-cartpole",
    title: "DQN on CartPole: Right Tool for the Job",
    body: `
Now look at the RIGHT panel for CartPole.

Here, DQN uses a neural network to approximate Q(s,a):

‚Ä¢ Inputs: the continuous state values
‚Ä¢ Hidden layers: learn useful features automatically
‚Ä¢ Outputs: Q-values for each action (left/right)

Because the network is continuous, it can generalize:
states that are ‚Äúsimilar‚Äù in angle/velocity correspond to similar Q-values.

Compare the reward-per-episode trend for DQN vs Q-Learning on CartPole:
DQN usually learns a more stable, high-reward policy.`,
    focusSide: "right",
    sectionId: "right-panel",
  },
  {
    id: "reward-curves",
    title: "Using Reward Curves to Judge Methods",
    body: `
Scroll down within each panel so you can see the reward-per-episode plots.

These curves summarize how well each method is doing:

‚Ä¢ Early episodes: low reward and high variance (agent is exploring)
‚Ä¢ As learning progresses:
  ‚Äì Good methods: trend goes up and stabilizes
  ‚Äì Poorly matched methods: trend is noisy or plateaus early

Try this experiment:

1) On CliffWalk:
   ‚Äì Compare Q-Learning vs DQN reward curves.
   ‚Äì Q-Learning often wins with simpler, cleaner learning.

2) On CartPole:
   ‚Äì Compare the curves again.
   ‚Äì DQN should typically achieve higher, more stable rewards.

This is how you can visually see when a method is ‚Äútoo simple‚Äù
or ‚Äútoo heavy‚Äù for a given task.`,
    focusSide: "both",
    sectionId: "panels-wrapper",
  },
  {
    id: "hands-on",
    title: "Hands-On: Explore Episodes and Steps",
    body: `
To really understand what the agents are doing:

1) Pick an environment (CliffWalk or CartPole).
2) Start at an early episode and scrub through the steps.
   ‚Äì Watch how often the agent fails or falls.
3) Jump to a late episode and scrub again.
   ‚Äì Look for more goal-directed, stable behavior.
4) Check how the update rule at the bottom changes Q(s,a) or the loss.

Use both sides together:

‚Ä¢ Ask: ‚ÄúWhat is tabular Q-Learning seeing and updating?‚Äù
‚Ä¢ Ask: ‚ÄúWhat is the DQN network learning and how fast?‚Äù`,
    focusSide: "both",
    sectionId: "panels-wrapper",
  },
  {
    id: "wrap-up",
    title: "Key Takeaways",
    body: `
You‚Äôve seen two core ideas:

1) What reinforcement learning is:
   ‚Ä¢ Agents learn from repeated interaction using reward signals.
   ‚Ä¢ They approximate a policy (directly or via Q(s,a)).

2) Why method choice matters:
   ‚Ä¢ Discrete, small problems (like CliffWalk):
     ‚Äì Simple tabular Q-Learning is efficient and interpretable.
   ‚Ä¢ Continuous or high-dimensional problems (like CartPole):
     ‚Äì Function approximation (DQN) is often necessary.

This interface is meant as a sandbox:
revisit episodes, switch environments, and compare curves to build
intuition for when each method makes sense.

You can reopen this tutorial anytime with the "Tutorial" button
in the top bar.`,
    focusSide: "both",
    sectionId: "panels-wrapper",
  },
];

function TutorialOverlay({ onClose, onFocusChange, environment }) {
  const [stepIndex, setStepIndex] = useState(0);
  const step = steps[stepIndex];
  const isFirst = stepIndex === 0;
  const isLast = stepIndex === steps.length - 1;

  const requiresCartpole = step.id === "switch-cartpole";
  const nextDisabled =
    !isLast && requiresCartpole && environment !== "cartpole";

  // Tell App which side to focus (left / right / both)
  useEffect(() => {
    if (!onFocusChange) return;
    onFocusChange(step.focusSide || "both");
  }, [step.focusSide, onFocusChange]);

  // Highlight the relevant DOM section by id
  useEffect(() => {
    const prev = document.querySelectorAll(".tutorial-highlight");
    prev.forEach((el) => el.classList.remove("tutorial-highlight"));

    if (step.sectionId) {
      const el = document.getElementById(step.sectionId);
      if (el) {
        el.classList.add("tutorial-highlight");
        el.scrollIntoView({ behavior: "smooth", block: "center" });
      }
    }

    return () => {
      const current = document.querySelectorAll(".tutorial-highlight");
      current.forEach((el) => el.classList.remove("tutorial-highlight"));
    };
  }, [step.sectionId]);

  const handleClose = () => {
    if (onFocusChange) onFocusChange(null);
    const current = document.querySelectorAll(".tutorial-highlight");
    current.forEach((el) => el.classList.remove("tutorial-highlight"));
    onClose?.();
  };

  // Keyboard navigation: ‚Üê ‚Üí Esc
  useEffect(() => {
    const handleKey = (e) => {
      if (e.key === "ArrowRight") {
        if (isLast) {
          handleClose();
        } else if (!nextDisabled) {
          setStepIndex((i) => i + 1);
        }
      } else if (e.key === "ArrowLeft") {
        if (!isFirst) {
          setStepIndex((i) => i - 1);
        }
      } else if (e.key === "Escape") {
        handleClose();
      }
    };

    window.addEventListener("keydown", handleKey);
    return () => window.removeEventListener("keydown", handleKey);
  }, [isFirst, isLast, nextDisabled]);

  // Position & dimming:
  //  - when focusSide === "left": overlay pushed RIGHT, no dim
  //  - when focusSide === "right": overlay pushed LEFT, no dim
  //  - when "both": centered with dimming
  const isSideFocused = step.focusSide === "left" || step.focusSide === "right";

  const backdropStyle = {
    position: "fixed",
    inset: 0,
    background: isSideFocused
      ? "transparent"
      : "rgba(15,23,42,0.45)",
    display: "flex",
    justifyContent:
      step.focusSide === "left"
        ? "flex-end" // show box on right, leave left clear
        : step.focusSide === "right"
        ? "flex-start" // show box on left, leave right clear
        : "center",
    alignItems: "center",
    padding: "5vh 3vw",
    zIndex: 12000,
    pointerEvents: "none",
  };

  const panelStyle = {
    width: "min(620px, 96vw)",
    maxHeight: "min(90vh, 580px)",
    background: "#ffffff",
    borderRadius: "18px",
    padding: "1.25rem 1.5rem 1.1rem",
    boxShadow: "0 18px 55px rgba(0,0,0,0.45)",
    fontFamily:
      "system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif",
    display: "flex",
    flexDirection: "column",
    position: "relative",
    pointerEvents: "auto",
    overflowY: "auto",
  };

  const titleStyle = {
    margin: 0,
    marginBottom: "0.6rem",
    fontSize: "clamp(1.1rem, 1.1vw + 0.95rem, 1.3rem)",
    fontWeight: 700,
    color: "#0f172a",
  };

  const bodyStyle = {
    fontSize: "clamp(0.87rem, 0.9vw + 0.65rem, 1rem)",
    lineHeight: 1.6,
    whiteSpace: "pre-line",
    color: "#111827",
  };

  const footerStyle = {
    marginTop: "1rem",
    display: "flex",
    alignItems: "center",
    justifyContent: "space-between",
    gap: "0.75rem",
  };

  const buttonBase = {
    borderRadius: "999px",
    border: "none",
    padding: "0.5rem 1rem",
    fontSize: "0.85rem",
    fontWeight: 600,
    cursor: "pointer",
  };

  const primaryButton = {
    ...buttonBase,
    background: "#2563eb",
    color: "white",
  };

  const secondaryButton = {
    ...buttonBase,
    background: "#e5e7eb",
    color: "#111827",
  };

  const skipButton = {
    position: "absolute",
    top: "0.6rem",
    right: "0.8rem",
    background: "transparent",
    border: "none",
    color: "#6b7280",
    fontSize: "0.8rem",
    cursor: "pointer",
  };

  return (
    <div style={backdropStyle}>
      <div style={panelStyle}>
        <button style={skipButton} onClick={handleClose}>
          Skip ‚úï
        </button>

        <h3 style={titleStyle}>{step.title}</h3>
        <p style={bodyStyle}>{step.body}</p>

        <div style={footerStyle}>
          <button
            style={{ ...secondaryButton, opacity: isFirst ? 0.5 : 1 }}
            onClick={() => !isFirst && setStepIndex((i) => i - 1)}
            disabled={isFirst}
          >
            ‚Üê Back
          </button>

          <div
            style={{
              fontSize: "0.8rem",
              color: "#6b7280",
              flex: 1,
              textAlign: "center",
            }}
          >
            Step {stepIndex + 1} / {steps.length}
          </div>

          <button
            style={{
              ...primaryButton,
              opacity: nextDisabled ? 0.6 : 1,
              cursor: nextDisabled ? "default" : "pointer",
            }}
            disabled={nextDisabled}
            onClick={() => {
              if (isLast) {
                handleClose();
              } else if (!nextDisabled) {
                setStepIndex((i) => i + 1);
              }
            }}
          >
            {requiresCartpole && environment !== "cartpole"
              ? "Switch to CartPole ‚Üë"
              : isLast
              ? "Start Exploring"
              : "Next ‚Üí"}
          </button>
        </div>

        {requiresCartpole && environment !== "cartpole" && (
          <div
            style={{
              marginTop: "0.35rem",
              fontSize: "0.78rem",
              color: "#b91c1c",
              textAlign: "right",
            }}
          >
            Use the "CartPole" tab at the top of the page to continue.
          </div>
        )}
      </div>
    </div>
  );
}

export default TutorialOverlay;